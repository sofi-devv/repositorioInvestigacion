{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NORMALIZACIÓN DEL DATASET\n",
    "\n",
    "Queremos que los resultados por parte del llm que sean distintos a los que se le mencionaron en el prompt se organicen en una categoría llamada \"inconsistente\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('./base_con_normalizacion.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El prompt 9 tuvo la particularidad de que asignó múltiples estiquetas a un mismo comentario, en este sentido, se creo una nueva columna que se llama **respuesta_prompt9_procesado** donde se le asigno a estos valores una doble categoría"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop  respuesta_prompt9\n",
    "data = data.drop(['respuesta_prompt9'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"respuesta_prompt9_procesado\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generación de vectores para prueba chi cuadrado en Wolfram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Asignar el DataFrame a una variable\n",
    "df = prompts\n",
    "\n",
    "# Crear 10 vectores con tamaño 100000\n",
    "diccionario_map = {'POSITIVO': 1, 'NEGATIVO': 2, 'NEUTRAL': 3, 'INCONSISTENTE': 4}\n",
    "\n",
    "vectores = {}\n",
    "for i in range(9, 11):\n",
    "    vector_nombre = f'X{i}'\n",
    "    vectores[vector_nombre] = df[f'prompt {i}'].replace(diccionario_map).values\n",
    "\n",
    "# Guardar los vectores en un archivo txt\n",
    "with open('vectores2.txt', 'w') as file:\n",
    "    for nombre, vector in vectores.items():\n",
    "        file.write(f\"{nombre} = {vector.tolist()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar los vectores pares en un archivo txt\n",
    "with open('vectores_pares.txt', 'w') as file_pares:\n",
    "    for i in range(2, 9, 2):\n",
    "        vector_nombre = f'X{i}'\n",
    "        file_pares.write(f\"{vector_nombre} = {{{', '.join(map(str, vectores[vector_nombre]))}}};\\n\")\n",
    "\n",
    "# Guardar los vectores impares en un archivo txt\n",
    "with open('vectores_impares.txt', 'w') as file_impares:\n",
    "    for i in range(1, 9, 2):\n",
    "        vector_nombre = f'X{i}'\n",
    "        file_impares.write(f\"{vector_nombre} = {{{', '.join(map(str, vectores[vector_nombre]))}}};\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = prompts\n",
    "# drop Texto Comentario\n",
    "df = df.drop(['Texto Comentario'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualización de coincidencias y clasificaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular los totales por fila\n",
    "totales = df[['POSITIVO', 'NEUTRAL', 'NEGATIVO', 'INCONSISTENTE']].sum(axis=1)\n",
    "\n",
    "# Crear una nueva tabla con el formato \"conteo (porcentaje)\"\n",
    "contingency_table_total = pd.DataFrame({\n",
    "    'POSITIVO': df['POSITIVO'].map(lambda x: f\"{x} ({(x / totales.sum() * 100):.2f}%)\"),\n",
    "    'NEUTRAL': df['NEUTRAL'].map(lambda x: f\"{x} ({(x / totales.sum() * 100):.2f}%)\"),\n",
    "    'NEGATIVO': df['NEGATIVO'].map(lambda x: f\"{x} ({(x / totales.sum() * 100):.2f}%)\"),\n",
    "}, index=df.index)\n",
    "\n",
    "# Imprimir la tabla con los conteos y porcentajes\n",
    "print(contingency_table_total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supongamos que tienes tus datos cargados en un DataFrame llamado 'df'\n",
    "contingency_table_total = pd.DataFrame({\n",
    "    'POSITIVO': (df == 'POSITIVO').sum(),\n",
    "    'NEUTRAL': (df == 'NEUTRAL').sum(),\n",
    "    'NEGATIVO': (df == 'NEGATIVO').sum(),\n",
    "\n",
    "})\n",
    "\n",
    "print(contingency_table_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular el total por columna (prompt)\n",
    "totales_por_prompt = df.apply(lambda col: col.value_counts())\n",
    "\n",
    "# Calcular los porcentajes por columna (prompt)\n",
    "porcentajes_por_prompt = totales_por_prompt.div(totales_por_prompt.sum(axis=0), axis=1) * 100\n",
    "\n",
    "# Combinar las cantidades y porcentajes en una tabla formateada\n",
    "contingency_table_total = totales_por_prompt.fillna(0).astype(int).astype(str) + \" (\" + porcentajes_por_prompt.fillna(0).round(2).astype(str) + \"%)\"\n",
    "\n",
    "# Reemplazar NaN por vacío (si alguna clasificación no está en un prompt)\n",
    "contingency_table_total = contingency_table_total.fillna(\"0 (0.00%)\")\n",
    "\n",
    "# transponer la tabla\n",
    "contingency_table_total = contingency_table_total.T\n",
    "\n",
    "contingency_table_total \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualización de inconsistencias entre respuestas de los prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtrar los inconsistentes\n",
    "prompts = prompts[~prompts.isin(['INCONSISTENCIA'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtra las filas donde al menos un valor en las columnas (excepto la columna 'No') es diferente\n",
    "filtered_prompts = prompts[prompts.drop(columns=['No']).apply(lambda row: len(set(row)) > 1, axis=1)]\n",
    "\n",
    "# Muestra el DataFrame filtrado\n",
    "filtered_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numero de filas de filtered_prompts\n",
    "\n",
    "len(filtered_prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matriz de proporción de coincidencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todas las respuestas que coincidan hacer una matriz de la proporción de coincidencias\n",
    "# para cada par de columnas\n",
    "import numpy as np\n",
    "\n",
    "coincidencias = np.zeros((prompts.shape[1], prompts.shape[1]))\n",
    "\n",
    "for i in range(prompts.shape[1]):\n",
    "    for j in range(prompts.shape[1]):\n",
    "        coincidencias[i, j] = (prompts.iloc[:, i] == prompts.iloc[:, j]).mean()\n",
    "\n",
    "coincidencias = pd.DataFrame(coincidencias, columns=prompts.columns, index=prompts.columns)\n",
    "\n",
    "coincidencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coincidencias.to_csv('./results/matrices/coincidencias_matrix.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis de embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import openai\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"API KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.getenv(api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    api_key=api_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function that takes a string and returns the embedding with comparacion[\"prompt\"]\n",
    "\n",
    "def get_embedding(text):\n",
    "    response = client.embeddings.create(\n",
    "        input=text,\n",
    "        model=\"text-embedding-3-small\"\n",
    "    )\n",
    "    return response.data[0].embedding\n",
    "\n",
    "# get the embeddings for each prompt\n",
    "comparacion[\"prompt_embedding\"] = comparacion[\"Prompt\"].apply(get_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparacion.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.array(comparacion[\"prompt_embedding\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make embeddings_pca a DataFrame an enumerate the columns with the prompts \n",
    "embeddings_pca = pd.DataFrame(embeddings_pca, columns=[\"PCA1\", \"PCA2\"])\n",
    "\n",
    "embeddings_pca[\"# \"] = comparacion[\"# \"]\n",
    "\n",
    "embeddings_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beautify the plot Prompt Similarity in PCA Space \n",
    "plt.figure()\n",
    "plt.scatter(embeddings_pca[\"PCA1\"], embeddings_pca[\"PCA2\"])\n",
    "\n",
    "for i, txt in enumerate(embeddings_pca[\"# \"]):\n",
    "    plt.annotate(txt, (embeddings_pca[\"PCA1\"][i], embeddings_pca[\"PCA2\"][i]))\n",
    "\n",
    "plt.title(\"Prompt Similarity in PCA Space\")\n",
    "plt.xlabel(\"PCA1\")\n",
    "plt.ylabel(\"PCA2\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# varianza explicada\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Creamos el scatter plot 3D\n",
    "fig = go.Figure(data=[go.Scatter3d(\n",
    "    x=embeddings_pca[:, 0],\n",
    "    y=embeddings_pca[:, 1],\n",
    "    z=embeddings_pca[:, 2],\n",
    "    mode='markers+text',\n",
    "    text=comparacion[\"# \"],  # Añade los textos a los puntos\n",
    "    marker=dict(\n",
    "        size=5,\n",
    "        color='blue',\n",
    "        opacity=0.8\n",
    "    )\n",
    ")])\n",
    "\n",
    "# Ajustamos el diseño\n",
    "fig.update_layout(\n",
    "    scene = dict(\n",
    "        xaxis_title='Componente 1',\n",
    "        yaxis_title='Componente 2',\n",
    "        zaxis_title='Componente 3'\n",
    "    ),\n",
    "    title='Scatter Plot 3D de Embeddings'\n",
    ")\n",
    "\n",
    "# Mostramos el gráfico interactivo\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Varianza acomulada de 71"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cálculo de correlación entre embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcular la correlación de los embeddings y graficar la matriz de correlación\n",
    "correlacion = np.corrcoef(embeddings)\n",
    "correlacion = pd.DataFrame(correlacion, columns=comparacion[\"# \"], index=comparacion[\"# \"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlacion.to_csv('./results/matrices/correlacion_matrix.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hacer la matriz de distancia euclidiana entre los embeddings\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "distancias = pdist(embeddings)\n",
    "\n",
    "distancias = squareform(distancias)\n",
    "\n",
    "distancias = pd.DataFrame(distancias, columns=comparacion[\"# \"], index=comparacion[\"# \"])\n",
    "\n",
    "distancias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kapa Kohen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import numpy as np\n",
    "\n",
    "def codificar_columnas(df):\n",
    "    \"\"\"\n",
    "    Codifica todas las columnas categóricas en números utilizando LabelEncoder.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    le = LabelEncoder()\n",
    "    \n",
    "    for column in df.columns:\n",
    "        df[column] = le.fit_transform(df[column].astype(str))  # Convertimos a string antes de codificar\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_kappa(df, columnas_excluir=None):\n",
    "    \"\"\"\n",
    "    Esta función calcula el coeficiente Kappa de Cohen para cada par de columnas.\n",
    "    \n",
    "    Args:\n",
    "    df (DataFrame): El DataFrame que contiene las respuestas de los evaluadores.\n",
    "    columnas_excluir (list): Lista de columnas a excluir del análisis, como la columna 'No' que representa IDs.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: Un DataFrame con los coeficientes Kappa para cada par de columnas.\n",
    "    \"\"\"\n",
    "    # Excluir las columnas indicadas en columnas_excluir\n",
    "    if columnas_excluir:\n",
    "        df = df.drop(columns=columnas_excluir, errors='ignore')\n",
    "    \n",
    "    # Codificar las columnas categóricas (si es necesario)\n",
    "    df = codificar_columnas(df)\n",
    "    \n",
    "    columnas = df.columns  # Las columnas restantes después de excluir 'No' u otras columnas\n",
    "    kappa_matriz = pd.DataFrame(np.zeros((len(columnas), len(columnas))), index=columnas, columns=columnas)\n",
    "    \n",
    "    # Iterar sobre todos los pares posibles de columnas\n",
    "    for i in range(len(columnas)):\n",
    "        for j in range(i + 1, len(columnas)):\n",
    "            col1 = columnas[i]\n",
    "            col2 = columnas[j]\n",
    "            \n",
    "            # Calcular el coeficiente Kappa de Cohen entre las dos columnas (evaluadores)\n",
    "            kappa = cohen_kappa_score(df[col1], df[col2])\n",
    "            \n",
    "            # Almacenar el valor de Kappa en la matriz simétrica\n",
    "            kappa_matriz.at[col1, col2] = kappa\n",
    "            kappa_matriz.at[col2, col1] = kappa  # La matriz es simétrica\n",
    "    \n",
    "    return kappa_matriz\n",
    "\n",
    "# Excluir la columna 'No', que representa IDs\n",
    "columnas_a_excluir = ['No']\n",
    "\n",
    "# Calcular solo el Kappa para todos los pares de prompts excluyendo 'No'\n",
    "kappa_matriz = calcular_kappa(prompts, columnas_a_excluir)\n",
    "\n",
    "# Mostrar la matriz de Kappa\n",
    "kappa_matriz\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imprimir una matriz con los resultados de Kappa\n",
    "kappa_matrix = pd.DataFrame(index=prompts.columns, columns=prompts.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def calcular_kappa_sin_paquete(df, columnas_excluir=None):\n",
    "    \"\"\"\n",
    "    Calcula el coeficiente Kappa de Cohen entre todas las columnas del DataFrame sin usar paquetes externos.\n",
    "    \n",
    "    Args:\n",
    "    df (DataFrame): El DataFrame que contiene las respuestas de los evaluadores.\n",
    "    columnas_excluir (list): Lista de columnas a excluir del análisis, como la columna 'No' que representa IDs.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: Un DataFrame con los coeficientes Kappa para cada par de columnas.\n",
    "    \"\"\"\n",
    "    # Excluir las columnas indicadas en columnas_excluir\n",
    "    if columnas_excluir:\n",
    "        df = df.drop(columns=columnas_excluir, errors='ignore')\n",
    "    \n",
    "    columnas = df.columns  # Las columnas restantes después de excluir las indicadas\n",
    "    kappa_matriz = pd.DataFrame(np.zeros((len(columnas), len(columnas))), index=columnas, columns=columnas)\n",
    "    \n",
    "    # Función auxiliar para calcular Kappa entre dos columnas (sin paquetes)\n",
    "    def calcular_kappa_col(col1, col2):\n",
    "        n = len(col1)\n",
    "        acuerdo_observado = sum([1 for i in range(n) if col1[i] == col2[i]]) / n\n",
    "        \n",
    "        unique_values = set(col1).union(set(col2))\n",
    "        frec_col1 = {val: col1.count(val) / n for val in unique_values}\n",
    "        frec_col2 = {val: col2.count(val) / n for val in unique_values}\n",
    "        \n",
    "        acuerdo_esperado = sum([frec_col1[val] * frec_col2[val] for val in unique_values])\n",
    "        \n",
    "        if acuerdo_esperado == 1:\n",
    "            return 1  # Si el acuerdo esperado es perfecto, el Kappa es 1\n",
    "        \n",
    "        kappa = (acuerdo_observado - acuerdo_esperado) / (1 - acuerdo_esperado)\n",
    "        return kappa\n",
    "    \n",
    "    # Iterar sobre todos los pares posibles de columnas\n",
    "    for i in range(len(columnas)):\n",
    "        for j in range(i, len(columnas)):  # Comparar columna consigo misma y con otras\n",
    "            if i == j:\n",
    "                kappa_matriz.iloc[i, j] = 1  # El Kappa de una columna consigo misma es 1\n",
    "            else:\n",
    "                col1 = df[columnas[i]].tolist()\n",
    "                col2 = df[columnas[j]].tolist()\n",
    "                kappa = calcular_kappa_col(col1, col2)\n",
    "                kappa_matriz.iloc[i, j] = kappa\n",
    "                kappa_matriz.iloc[j, i] = kappa  # La matriz es simétrica\n",
    "    \n",
    "    return kappa_matriz\n",
    "\n",
    "# Excluir la columna 'No', que representa IDs\n",
    "columnas_a_excluir = ['No']\n",
    "\n",
    "# Calcular solo el Kappa para todos los pares de prompts excluyendo 'No'\n",
    "kappa_matriz = calcular_kappa_sin_paquete(prompts, columnas_a_excluir)\n",
    "\n",
    "# Mostrar la matriz de Kappa\n",
    "kappa_matriz\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pwd  # Esto muestra el directorio actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd  # Esto muestra el directorio actual\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa_matriz.to_csv('./results/matrices/kappa_matrix.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matriz de proporciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todas las respuestas que coincidan hacer una matriz de la proporción de coincidencias\n",
    "# para cada par de columnas\n",
    "import numpy as np\n",
    "\n",
    "coincidencias = np.zeros((prompts.shape[1], prompts.shape[1]))\n",
    "\n",
    "for i in range(prompts.shape[1]):\n",
    "    for j in range(prompts.shape[1]):\n",
    "        coincidencias[i, j] = (prompts.iloc[:, i] == prompts.iloc[:, j]).mean()\n",
    "\n",
    "coincidencias = pd.DataFrame(coincidencias, columns=prompts.columns, index=prompts.columns)\n",
    "\n",
    "coincidencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coincidencias.to_csv('./results/matrices/coincidencias_matrix.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
